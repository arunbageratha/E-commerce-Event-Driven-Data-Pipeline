# ğŸ›ï¸ E-commerce Event-Driven Data Pipeline

## ğŸ§¾ Project Overview
This project implements a **comprehensive event-driven data pipeline** for e-commerce transactional data processing using **Databricks**, **PySpark**, and **Delta Lake**.  
It handles multiple data sources with advanced **data engineering patterns** such as **SCD2 (Slowly Changing Dimensions)**, **data validation**, **data enrichment**, and **automated archiving**.

---

## âš™ï¸ Tech Stack
- ğŸ’» **Databricks** â€“ Cloud-based data processing platform  
- ğŸ”¥ **PySpark** â€“ Distributed data processing engine  
- ğŸ§Š **Delta Lake** â€“ ACID transactions and versioning for data lakes  
- ğŸ“¦ **Databricks Volumes** â€“ Managed storage for file-based data sources  
- ğŸ§  **Databricks Workflows** â€“ Job orchestration and scheduling  
- ğŸ§° **GitHub** â€“ Version control and CI/CD integration  

---

## ğŸ—‚ï¸ Project Structure

Ecomm_Event_Driven_Datapipeline/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ customers_2025_01_15.csv
â”‚   â”œâ”€â”€ customers_2025_01_16.csv
â”‚   â”œâ”€â”€ inventory_2025_01_15.csv
â”‚   â”œâ”€â”€ inventory_2025_01_16.csv
â”‚   â”œâ”€â”€ orders_2025_01_15.csv
â”‚   â”œâ”€â”€ orders_2025_01_16.csv
â”‚   â”œâ”€â”€ products_2025_01_15.csv
â”‚   â”œâ”€â”€ products_2025_01_16.csv
â”‚   â”œâ”€â”€ shipping_2025_01_15.csv
â”‚   â”œâ”€â”€ shipping_2025_01_16.csv
â”‚   â”œâ”€â”€ trigger_2025_01_15.json
â”‚   â””â”€â”€ trigger_2025_01_16.json
â””â”€â”€ notebook/
    â”œâ”€â”€ 01_orders_stage_load.ipynb
    â”œâ”€â”€ 02_customers_stage_load.ipynb
    â”œâ”€â”€ 03_products_stage_load.ipynb
    â”œâ”€â”€ 04_inventory_stage_load.ipynb
    â”œâ”€â”€ 05_shipping_stage_load.ipynb
    â”œâ”€â”€ 06_data_validation.ipynb
    â”œâ”€â”€ 07_data_enrichment.ipynb
    â””â”€â”€ 08_final_merge_operation.ipynb

---

## ğŸ§± Pipeline Architecture

### ğŸ”¹ 1. Data Ingestion Layer
- ğŸ“„ **Source Files**: CSV files for orders, customers, products, inventory, and shipping  
- ğŸ§¾ **Trigger Files**: JSON files that initiate batch processing automatically  
- ğŸ—ƒï¸ **Storage**: Databricks Volumes for managed file storage  

### ğŸ”¹ 2. Data Processing Layer
- âš™ï¸ **Stage Load Notebooks**: Load and validate raw data  
- ğŸ§® **Validation Notebook**: Apply cross-reference and business rules  
- âœ¨ **Enrichment Notebook**: Add business metrics and analytics  
- ğŸ” **Merge Notebook**: Apply SCD2 for historical versioning  

### ğŸ”¹ 3. Data Storage Layer
- ğŸ§± **Staging Tables**: Temporary validated data  
- ğŸ§¾ **Target Tables**: Historical versioned data using Delta Lake  
- ğŸ“Š **Analytics Tables**: For BI and reporting  

---

## ğŸ“˜ Notebook Details

### ğŸ§¾ 01_orders_stage_load.ipynb
- Loads order data  
- Validates order amounts, customer & product IDs  
- Archives processed files  
- Logs processing summary  

### ğŸ‘¥ 02_customers_stage_load.ipynb
- Processes customer demographic data  
- Validates email & phone formats  
- Calculates customer age and lifecycle stage  
- Enriches with customer segments  

### ğŸ“¦ 03_products_stage_load.ipynb
- Loads product catalog data  
- Validates product IDs and pricing  
- Handles product hierarchy and categories  

### ğŸ¬ 04_inventory_stage_load.ipynb
- Tracks stock availability  
- Detects anomalies in stock levels  
- Updates warehouse-level inventory  

### ğŸšš 05_shipping_stage_load.ipynb
- Loads shipment and delivery data  
- Tracks order status, delays, and carrier details  

### ğŸ§ª 06_data_validation.ipynb
- Cross-validates across tables (e.g., Order-Customer-Product links)  
- Ensures referential integrity  
- Logs failed validations to an audit table  

### âœ¨ 07_data_enrichment.ipynb
- Adds calculated KPIs like total revenue, margin, delivery SLA  
- Joins with product and customer attributes for analytics  

### ğŸ” 08_final_merge_operation.ipynb
- Implements **SCD Type 2** for historical tracking  
- Merges incremental data into master Delta tables  
- Updates audit logs and archival layers  

---

## ğŸ§© Key Features
- ğŸ”” **Event-driven** batch processing with trigger JSONs  
- ğŸ§¾ **Schema validation** and error handling  
- ğŸ§  **SCD2 history tracking** for dimension tables  
- ğŸ“‚ **Automatic archiving** of processed source files  
- ğŸ”„ **Reprocessing support** for failed batches  
- ğŸ§° **Fully orchestrated via Databricks Workflows**

---

## ğŸš€ Future Enhancements
- ğŸª£ Migrate file ingestion to **Autoloader** for streaming  
- ğŸ§  Add **ML model** for sales prediction  
- ğŸ§¾ Implement **Data Quality checks** using **Deequ or Great Expectations**  
- â˜ï¸ Integrate with **Power BI or Qlik** for visualization  

---

## ğŸ Summary
This pipeline demonstrates how **Databricks + PySpark + Delta Lake** can be used to build a **scalable, maintainable, and event-driven data architecture** for e-commerce platforms.

